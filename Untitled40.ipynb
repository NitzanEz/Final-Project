{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitzanEz/Final-Project/blob/main/Untitled40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9kaonsoorxF",
        "outputId": "57e1e362-b3cd-48dd-fbbc-cb5998edd6e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Final-Project'...\n",
            "remote: Enumerating objects: 114207, done.\u001b[K\n",
            "remote: Counting objects: 100% (1815/1815), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1776/1776), done.\u001b[K\n",
            "remote: Total 114207 (delta 97), reused 1108 (delta 37), pack-reused 112392 (from 9)\u001b[K\n",
            "Receiving objects: 100% (114207/114207), 2.42 GiB | 12.48 MiB/s, done.\n",
            "Resolving deltas: 100% (1544/1544), done.\n",
            "Updating files: 100% (58451/58451), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NitzanEz/Final-Project.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/Final-Project')\n",
        "\n",
        "from inception_v4 import create_model  # Make sure this script is correctly formatted and located\n",
        "\n",
        "# Directory setup\n",
        "train_dir = '/content/Final-Project/Data/train'\n",
        "validation_dir = '/content/Final-Project/Data/validation'\n",
        "test_dir = '/content/Final-Project/Data/test'\n",
        "\n",
        "# Model parameters\n",
        "img_width, img_height = 299, 299\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "# Load the pre-trained Inception V4 model without the top layer (for transfer learning)\n",
        "model = create_model(num_classes=1, dropout_prob=0.3, weights='imagenet', include_top=False)\n",
        "\n",
        "# Adding custom Layers\n",
        "x = model.output\n",
        "x = MaxPooling2D(pool_size=(8, 8))(x)  # Adjust pool_size if needed\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up the ModelCheckpoint callback to save only the best model based on validation accuracy\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Use image_dataset_from_directory to load images\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary')\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    image_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='binary')\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=[checkpoint]  # Add the checkpoint callback here\n",
        ")\n",
        "\n",
        "# Load the best model saved during training\n",
        "best_model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "val_loss, val_acc = best_model.evaluate(validation_dataset)\n",
        "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5QfAt1AkIiS",
        "outputId": "85952096-b007-42f0-a07e-655aeaa47134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 33910 files belonging to 2 classes.\n",
            "Found 18077 files belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 235ms/step - accuracy: 0.5399 - loss: 3.3919 - val_accuracy: 0.4932 - val_loss: 7390.1167\n",
            "Epoch 2/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.6686 - loss: 2.7670 - val_accuracy: 0.4910 - val_loss: 5.8120\n",
            "Epoch 3/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.7607 - loss: 2.2403 - val_accuracy: 0.4942 - val_loss: 29.6941\n",
            "Epoch 4/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.8410 - loss: 1.8051 - val_accuracy: 0.4932 - val_loss: 2767.9333\n",
            "Epoch 5/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.7881 - loss: 1.6443 - val_accuracy: 0.5130 - val_loss: 11.1896\n",
            "Epoch 6/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9020 - loss: 1.2265 - val_accuracy: 0.4932 - val_loss: 77.5388\n",
            "Epoch 7/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9265 - loss: 1.0294 - val_accuracy: 0.5068 - val_loss: 272.9175\n",
            "Epoch 8/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9358 - loss: 0.8722 - val_accuracy: 0.4932 - val_loss: 87.7794\n",
            "Epoch 9/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 134ms/step - accuracy: 0.9351 - loss: 0.7732 - val_accuracy: 0.4831 - val_loss: 41.6700\n",
            "Epoch 10/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9417 - loss: 0.6887 - val_accuracy: 0.5068 - val_loss: 10350.1221\n",
            "Epoch 11/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9497 - loss: 0.6196 - val_accuracy: 0.4971 - val_loss: 52.6915\n",
            "Epoch 12/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9578 - loss: 0.5433 - val_accuracy: 0.4932 - val_loss: 115.6011\n",
            "Epoch 13/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9614 - loss: 0.5011 - val_accuracy: 0.5141 - val_loss: 7.6272\n",
            "Epoch 14/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9617 - loss: 0.4895 - val_accuracy: 0.5315 - val_loss: 39.4088\n",
            "Epoch 15/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9690 - loss: 0.4458 - val_accuracy: 0.5312 - val_loss: 72.1236\n",
            "Epoch 16/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9640 - loss: 0.4386 - val_accuracy: 0.4936 - val_loss: 3.6193\n",
            "Epoch 17/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9747 - loss: 0.4038 - val_accuracy: 0.5010 - val_loss: 1.0506\n",
            "Epoch 18/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9737 - loss: 0.3770 - val_accuracy: 0.5063 - val_loss: 8.0272\n",
            "Epoch 19/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9754 - loss: 0.3739 - val_accuracy: 0.4809 - val_loss: 44.6693\n",
            "Epoch 20/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9769 - loss: 0.3675 - val_accuracy: 0.5046 - val_loss: 2.4178\n",
            "Epoch 21/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9801 - loss: 0.3271 - val_accuracy: 0.5040 - val_loss: 0.9607\n",
            "Epoch 22/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 135ms/step - accuracy: 0.9816 - loss: 0.3080 - val_accuracy: 0.5116 - val_loss: 0.9828\n",
            "Epoch 23/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9781 - loss: 0.3089 - val_accuracy: 0.5112 - val_loss: 1.0848\n",
            "Epoch 24/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9642 - loss: 0.3736 - val_accuracy: 0.5068 - val_loss: 46.3043\n",
            "Epoch 25/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 136ms/step - accuracy: 0.9834 - loss: 0.2967 - val_accuracy: 0.5229 - val_loss: 0.9567\n",
            "Epoch 26/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 136ms/step - accuracy: 0.9826 - loss: 0.2830 - val_accuracy: 0.5068 - val_loss: 1.2151\n",
            "Epoch 27/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9813 - loss: 0.2807 - val_accuracy: 0.5068 - val_loss: 1.1045\n",
            "Epoch 28/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9870 - loss: 0.2582 - val_accuracy: 0.4932 - val_loss: 198877.7500\n",
            "Epoch 29/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9878 - loss: 0.2597 - val_accuracy: 0.5040 - val_loss: 1.7756\n",
            "Epoch 30/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9876 - loss: 0.2462 - val_accuracy: 0.4947 - val_loss: 5.0271\n",
            "Epoch 31/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9869 - loss: 0.2478 - val_accuracy: 0.5039 - val_loss: 1.0756\n",
            "Epoch 32/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9818 - loss: 0.2676 - val_accuracy: 0.5068 - val_loss: 1.1855\n",
            "Epoch 33/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9882 - loss: 0.2311 - val_accuracy: 0.5068 - val_loss: 1.5429\n",
            "Epoch 34/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 133ms/step - accuracy: 0.9882 - loss: 0.2455 - val_accuracy: 0.5285 - val_loss: 17.8852\n",
            "Epoch 35/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9898 - loss: 0.2313 - val_accuracy: 0.5209 - val_loss: 1.1383\n",
            "Epoch 36/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9897 - loss: 0.2140 - val_accuracy: 0.5068 - val_loss: 1.1706\n",
            "Epoch 37/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 134ms/step - accuracy: 0.9908 - loss: 0.2109 - val_accuracy: 0.5068 - val_loss: 1.0429\n",
            "Epoch 38/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9876 - loss: 0.2168 - val_accuracy: 0.5063 - val_loss: 0.9768\n",
            "Epoch 39/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9894 - loss: 0.2097 - val_accuracy: 0.5068 - val_loss: 1.1199\n",
            "Epoch 40/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9913 - loss: 0.1976 - val_accuracy: 0.5068 - val_loss: 1.3066\n",
            "Epoch 41/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 135ms/step - accuracy: 0.9904 - loss: 0.2076 - val_accuracy: 0.5234 - val_loss: 0.9453\n",
            "Epoch 42/50\n",
            "\u001b[1m1060/1060\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 136ms/step - accuracy: 0.9907 - loss: 0.1950 - val_accuracy: 0.5320 - val_loss: 1.7960\n",
            "Epoch 43/50\n",
            "\u001b[1m  17/1060\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 115ms/step - accuracy: 0.9817 - loss: 0.2121"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss together\n",
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot training loss separately\n",
        "def plot_train_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot validation loss separately\n",
        "def plot_validation_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Call the functions to plot the graphs\n",
        "plot_loss(history)\n",
        "plot_train_loss(history)\n",
        "plot_validation_loss(history)\n"
      ],
      "metadata": {
        "id": "9Mb3MQyUm516"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM5Gb76cYHK1InUDVD/IDKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
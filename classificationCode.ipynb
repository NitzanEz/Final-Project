{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitzanEz/Final-Project/blob/main/classificationCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Pze-76cY2Q"
      },
      "source": [
        "Cloning the project repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lTbd9pey_aqf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NitzanEz/Final-Project.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAwdxv6Dcp7r"
      },
      "source": [
        "Import necessary libraries and define preprocessing and convolutional operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d7VGuYDNao_5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import MaxPooling2D, Conv2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
        "from tensorflow.keras.layers import BatchNormalization, concatenate\n",
        "from tensorflow.keras import regularizers, initializers, Model\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Normalization\n",
        "\n",
        "\n",
        "#########################################################################################\n",
        "# Implements the Inception Network v4 (http://arxiv.org/pdf/1602.07261v1.pdf) in Keras. #\n",
        "#########################################################################################\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/kentsommer/keras-inceptionV4/releases/download/2.1/inception-v4_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/kentsommer/keras-inceptionV4/releases/download/2.1/inception-v4_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "def preprocess_input(x):\n",
        "\n",
        "    x = np.divide(x, 255.0)\n",
        "    x = np.subtract(x, 0.5)\n",
        "    x = np.multiply(x, 2.0)\n",
        "    return x\n",
        "\n",
        "def conv2d_bn(x, nb_filter, num_row, num_col, padding='same', strides=(1, 1), use_bias=False):\n",
        "    \"\"\"\n",
        "    Utility function to apply conv + BN.\n",
        "    \"\"\"\n",
        "    channel_axis = -1 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "    x = Conv2D(nb_filter, (num_row, num_col),\n",
        "               strides=strides,\n",
        "               padding=padding,\n",
        "               use_bias=use_bias,\n",
        "               kernel_regularizer=regularizers.l2(0.00004),\n",
        "               kernel_initializer=initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None))(x)\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.9997, scale=False)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def block_inception_a(input):\n",
        "    channel_axis = -1 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 96, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 64, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 96, 3, 3)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 64, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 96, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "def block_reduction_a(input):\n",
        "    channel_axis = -1 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 384, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 224, 3, 3)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_2 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(input)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "def block_inception_b(input):\n",
        "    channel_axis = -1 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 384, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 224, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 7, 1)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 192, 7, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 224, 1, 7)\n",
        "    branch_2 = conv2d_bn(branch_2, 224, 7, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 256, 1, 7)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 128, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "def block_reduction_b(input):\n",
        "    channel_axis = -1 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 192, 3, 3, strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 256, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 320, 7, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 320, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(input)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "def block_inception_c(input):\n",
        "    channel_axis = -1 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 256, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 384, 1, 1)\n",
        "    branch_10 = conv2d_bn(branch_1, 256, 1, 3)\n",
        "    branch_11 = conv2d_bn(branch_1, 256, 3, 1)\n",
        "    branch_1 = concatenate([branch_10, branch_11], axis=channel_axis)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 384, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 448, 3, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 512, 1, 3)\n",
        "    branch_20 = conv2d_bn(branch_2, 256, 1, 3)\n",
        "    branch_21 = conv2d_bn(branch_2, 256, 3, 1)\n",
        "    branch_2 = concatenate([branch_20, branch_21], axis=channel_axis)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 256, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "def inception_v4_base(input):\n",
        "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    norm_layer = Normalization(axis=-1)\n",
        "\n",
        "    # Apply normalization to the input tensor\n",
        "    x = norm_layer(input)\n",
        "\n",
        "    # Proceed with the model architecture\n",
        "    x = conv2d_bn(x, 32, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    x = conv2d_bn(x, 32, 3, 3, strides=(2, 2), padding='valid')\n",
        "    net = conv2d_bn(input, 32, 3, 3, strides=(2,2), padding='valid')\n",
        "    net = conv2d_bn(net, 32, 3, 3, padding='valid')\n",
        "    net = conv2d_bn(net, 64, 3, 3)\n",
        "\n",
        "    branch_0 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
        "\n",
        "    branch_1 = conv2d_bn(net, 96, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    branch_0 = conv2d_bn(net, 64, 1, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 96, 3, 3, padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(net, 64, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 7, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 96, 3, 3, padding='valid')\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    branch_0 = conv2d_bn(net, 192, 3, 3, strides=(2,2), padding='valid')\n",
        "    branch_1 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    # 4 x Inception-A blocks\n",
        "    for i in range(4):\n",
        "        net = block_inception_a(net)\n",
        "\n",
        "    # Reduction-A block\n",
        "    net = block_reduction_a(net)\n",
        "\n",
        "    # 7 x Inception-B blocks\n",
        "    for i in range(7):\n",
        "        net = block_inception_b(net)\n",
        "\n",
        "    # Reduction-B block\n",
        "    net = block_reduction_b(net)\n",
        "\n",
        "    # 3 x Inception-C blocks\n",
        "    for i in range(3):\n",
        "        net = block_inception_c(net)\n",
        "\n",
        "    return net\n",
        "\n",
        "def inception_v4(num_classes, dropout_keep_prob, weights, include_top):\n",
        "    inputs = Input((299, 299, 3))\n",
        "    x = inception_v4_base(inputs)\n",
        "\n",
        "    if include_top:\n",
        "        x = AveragePooling2D((8, 8), padding='valid')(x)\n",
        "        x = Dropout(dropout_keep_prob)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(units=num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, x, name='inception_v4')\n",
        "\n",
        "    if weights == 'imagenet':\n",
        "        if include_top:\n",
        "            weights_path = get_file('inception-v4_weights_tf_dim_ordering_tf_kernels.h5', WEIGHTS_PATH, cache_subdir='models')\n",
        "        else:\n",
        "            weights_path = get_file('inception-v4_weights_tf_dim_ordering_tf_kernels_notop.h5', WEIGHTS_PATH_NO_TOP, cache_subdir='models')\n",
        "        model.load_weights(weights_path, by_name=True)\n",
        "    return model\n",
        "\n",
        "def create_and_compile_model(num_classes=1001, dropout_prob=0.2, weights='imagenet', include_top=True, learning_rate=0.0001):\n",
        "    model = inception_v4(num_classes, dropout_prob, weights, include_top)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Usage\n",
        "model = create_and_compile_model(num_classes=1001, dropout_prob=0.2, weights='imagenet', include_top=True, learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtI3ZFnDcwSK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHBFsHdDKEcf",
        "outputId": "056aac5c-1717-4dd2-e5e8-f4011d03d566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 24056 images belonging to 2 classes.\n",
            "Found 21939 images belonging to 2 classes.\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 295ms/step - accuracy: 0.5398 - loss: 2.9656 - val_accuracy: 0.4394 - val_loss: 7652.7769\n",
            "Epoch 2/50\n",
            "\u001b[1m  1/751\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 134ms/step - accuracy: 0.7812 - loss: 1.9412"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.7812 - loss: 1.9412 - val_accuracy: 0.5789 - val_loss: 5430.8745\n",
            "Epoch 3/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 179ms/step - accuracy: 0.6480 - loss: 1.8213 - val_accuracy: 0.4397 - val_loss: 85.2958\n",
            "Epoch 4/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - accuracy: 0.7500 - loss: 1.4050 - val_accuracy: 0.2105 - val_loss: 88.3215\n",
            "Epoch 5/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 177ms/step - accuracy: 0.7122 - loss: 1.3425 - val_accuracy: 0.4394 - val_loss: 33279.9023\n",
            "Epoch 6/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78us/step - accuracy: 0.6875 - loss: 1.2102 - val_accuracy: 0.5263 - val_loss: 28705.3750\n",
            "Epoch 7/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 176ms/step - accuracy: 0.7586 - loss: 1.0502 - val_accuracy: 0.4394 - val_loss: 9954.9482\n",
            "Epoch 8/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - accuracy: 0.8125 - loss: 0.8775 - val_accuracy: 0.5263 - val_loss: 8265.0977\n",
            "Epoch 9/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 179ms/step - accuracy: 0.8109 - loss: 0.8312 - val_accuracy: 0.4396 - val_loss: 10411.8750\n",
            "Epoch 10/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - accuracy: 0.8438 - loss: 0.6437 - val_accuracy: 0.5789 - val_loss: 16088.0391\n",
            "Epoch 11/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.8411 - loss: 0.6986 - val_accuracy: 0.4640 - val_loss: 3059.6111\n",
            "Epoch 12/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - accuracy: 0.9688 - loss: 0.4767 - val_accuracy: 0.4211 - val_loss: 2051.2285\n",
            "Epoch 13/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.8729 - loss: 0.5837 - val_accuracy: 0.4391 - val_loss: 5921.8604\n",
            "Epoch 14/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81us/step - accuracy: 0.9062 - loss: 0.5552 - val_accuracy: 0.5263 - val_loss: 20197.0703\n",
            "Epoch 15/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9044 - loss: 0.4882 - val_accuracy: 0.4394 - val_loss: 2136.6226\n",
            "Epoch 16/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.9375 - loss: 0.4243 - val_accuracy: 0.5263 - val_loss: 2207.9639\n",
            "Epoch 17/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 179ms/step - accuracy: 0.9227 - loss: 0.4232 - val_accuracy: 0.5606 - val_loss: 161.8915\n",
            "Epoch 18/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - accuracy: 0.9375 - loss: 0.4710 - val_accuracy: 0.4737 - val_loss: 199.3990\n",
            "Epoch 19/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9336 - loss: 0.3926 - val_accuracy: 0.4395 - val_loss: 177.8856\n",
            "Epoch 20/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.9688 - loss: 0.3659 - val_accuracy: 0.4211 - val_loss: 215.0963\n",
            "Epoch 21/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9409 - loss: 0.3693 - val_accuracy: 0.4395 - val_loss: 205.4577\n",
            "Epoch 22/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - accuracy: 0.8438 - loss: 0.5200 - val_accuracy: 0.4737 - val_loss: 134.1109\n",
            "Epoch 23/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9488 - loss: 0.3419 - val_accuracy: 0.4396 - val_loss: 3080.7205\n",
            "Epoch 24/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.9375 - loss: 0.3677 - val_accuracy: 0.2632 - val_loss: 3029.7175\n",
            "Epoch 25/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9539 - loss: 0.3262 - val_accuracy: 0.4393 - val_loss: 413.4274\n",
            "Epoch 26/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - accuracy: 1.0000 - loss: 0.2584 - val_accuracy: 0.6316 - val_loss: 199.7016\n",
            "Epoch 27/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 177ms/step - accuracy: 0.9561 - loss: 0.3165 - val_accuracy: 0.4396 - val_loss: 732.2920\n",
            "Epoch 28/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.9688 - loss: 0.2304 - val_accuracy: 0.3684 - val_loss: 913.6167\n",
            "Epoch 29/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 177ms/step - accuracy: 0.9601 - loss: 0.3015 - val_accuracy: 0.4405 - val_loss: 35.1964\n",
            "Epoch 30/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79us/step - accuracy: 1.0000 - loss: 0.2190 - val_accuracy: 0.5263 - val_loss: 33.0413\n",
            "Epoch 31/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9582 - loss: 0.3037 - val_accuracy: 0.4388 - val_loss: 6.7983\n",
            "Epoch 32/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - accuracy: 0.8438 - loss: 0.4837 - val_accuracy: 0.4737 - val_loss: 4.6065\n",
            "Epoch 33/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 179ms/step - accuracy: 0.9720 - loss: 0.2702 - val_accuracy: 0.4420 - val_loss: 5.3098\n",
            "Epoch 34/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.8750 - loss: 0.5921 - val_accuracy: 0.5789 - val_loss: 7.2193\n",
            "Epoch 35/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9664 - loss: 0.2924 - val_accuracy: 0.5009 - val_loss: 0.9873\n",
            "Epoch 36/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 1.0000 - loss: 0.2085 - val_accuracy: 0.5263 - val_loss: 0.9101\n",
            "Epoch 37/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9698 - loss: 0.2709 - val_accuracy: 0.4513 - val_loss: 1.7340\n",
            "Epoch 38/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - accuracy: 0.9688 - loss: 0.2577 - val_accuracy: 0.5789 - val_loss: 0.9152\n",
            "Epoch 39/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9741 - loss: 0.2648 - val_accuracy: 0.4396 - val_loss: 2.5912\n",
            "Epoch 40/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.9688 - loss: 0.2457 - val_accuracy: 0.4737 - val_loss: 2.0956\n",
            "Epoch 41/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 177ms/step - accuracy: 0.9741 - loss: 0.2646 - val_accuracy: 0.4394 - val_loss: 113.2826\n",
            "Epoch 42/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - accuracy: 1.0000 - loss: 0.2210 - val_accuracy: 0.5789 - val_loss: 76.3849\n",
            "Epoch 43/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9744 - loss: 0.2714 - val_accuracy: 0.4396 - val_loss: 2.1030\n",
            "Epoch 44/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73us/step - accuracy: 1.0000 - loss: 0.1851 - val_accuracy: 0.5263 - val_loss: 1.8288\n",
            "Epoch 45/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9760 - loss: 0.2503 - val_accuracy: 0.4396 - val_loss: 1.7016\n",
            "Epoch 46/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - accuracy: 1.0000 - loss: 0.2183 - val_accuracy: 0.2632 - val_loss: 1.8392\n",
            "Epoch 47/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9785 - loss: 0.2518 - val_accuracy: 0.4395 - val_loss: 3.4971\n",
            "Epoch 48/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 1.0000 - loss: 0.2282 - val_accuracy: 0.4211 - val_loss: 3.5818\n",
            "Epoch 49/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 178ms/step - accuracy: 0.9826 - loss: 0.2296 - val_accuracy: 0.5607 - val_loss: 2.5463\n",
            "Epoch 50/50\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - accuracy: 0.9375 - loss: 0.2559 - val_accuracy: 0.4737 - val_loss: 2.1612\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'load_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bd7a5c4fc31e>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Optionally, you can reload and use any of the saved models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(299, 299, 3))\n",
        "base_model_output = inception_v4_base(inputs)\n",
        "x = GlobalAveragePooling2D()(base_model_output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, predictions)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_dir = '/content/Final-Project/Data/train'\n",
        "validation_dir = '/content/Final-Project/Data/validation'\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "model.save('final_model.keras')\n",
        "\n",
        "final_model = load_model('final_model.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YA34yoCP-WqB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image as keras_image\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "\n",
        "# Load your Inception v4 model\n",
        "model = load_model('/content/best_model.keras')  # Correct path to your model\n",
        "\n",
        "def preprocess_input(x):\n",
        "    \"\"\"Preprocess the input image for Inception v4.\"\"\"\n",
        "    x = np.divide(x, 255.0)  # Normalize to [0, 1]\n",
        "    x = np.subtract(x, 0.5)  # Center to [-0.5, 0.5]\n",
        "    x = np.multiply(x, 2.0)  # Scale to [-1, 1]\n",
        "    return x\n",
        "\n",
        "def load_and_pad_image(file_path, target_size=(299, 299)):\n",
        "    \"\"\"Load an image and pad it to the target size.\"\"\"\n",
        "    img = keras_image.load_img(file_path)\n",
        "    img = img.resize(target_size, Image.Resampling.LANCZOS)  # Use LANCZOS for high-quality downsampling\n",
        "    img_array = keras_image.img_to_array(img)\n",
        "    img_array = preprocess_input(img_array)  # Custom preprocessing for Inception v4\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "# Path to your image\n",
        "image_path = '/content/Final-Project/Data/test/ASD/28872/slice_136.jpg'  # Correct path to your image\n",
        "\n",
        "# Load and pad the image\n",
        "prepared_image = load_and_pad_image(image_path)\n",
        "\n",
        "# Predict using the model\n",
        "predictions = model.predict(prepared_image)\n",
        "predicted_class = np.argmax(predictions, axis=1)  # Assuming your model outputs logits for each class\n",
        "\n",
        "# Map the class indices back to class labels if necessary\n",
        "class_labels = {0: 'Non-ASD', 1: 'ASD'}\n",
        "predicted_label = class_labels[predicted_class[0]]\n",
        "\n",
        "print(f\"The image is classified as: {predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jo8GtYkeVCvg"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Try to open an image file directly with PIL\n",
        "try:\n",
        "    img = Image.open('path_to_problematic_image.jpg')\n",
        "    img.show()  # This will display the image if it can be opened\n",
        "except IOError:\n",
        "    print(\"Cannot open image\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DmPkH9q26phl"
      },
      "outputs": [],
      "source": [
        "!git add classificationCode.ipynb  # Change 'Notebook.ipynb' to your file name\n",
        "!git commit -m \"Add Jupyter notebook\"\n",
        "!git push\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitzanEz/Final-Project/blob/main/Untitled41.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1wrz-UP3AlJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCzKFX0D23dG",
        "outputId": "023b6533-da78-426a-ad48-3abc42f30be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Final-Project'...\n",
            "remote: Enumerating objects: 122817, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 122817 (delta 42), reused 14 (delta 14), pack-reused 122707 (from 4)\u001b[K\n",
            "Receiving objects: 100% (122817/122817), 2.93 GiB | 54.11 MiB/s, done.\n",
            "Resolving deltas: 100% (1804/1804), done.\n",
            "Updating files: 100% (44267/44267), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NitzanEz/Final-Project.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgFJ5I514R29",
        "outputId": "c4990126-b771-465e-eef8-cb9650b41d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (if not already installed)\n",
        "!pip install torch torchvision torchaudio timm\n",
        "!pip install transformers\n",
        "!pip install matplotlib  # For plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKDjIMjI6nMe",
        "outputId": "61ca2a0e-f066-40e4-94e7-ca8f478c5278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0\n",
            "--------------------------------\n",
            "Epoch [1/150]: Train Loss: 0.8574, Train Acc: 64.00%, Val Loss: 0.8205, Val Acc: 64.21%\n",
            "Epoch [2/150]: Train Loss: 0.7979, Train Acc: 66.02%, Val Loss: 0.7802, Val Acc: 67.55%\n",
            "Epoch [3/150]: Train Loss: 0.7701, Train Acc: 68.31%, Val Loss: 0.7537, Val Acc: 69.53%\n",
            "Epoch [4/150]: Train Loss: 0.7519, Train Acc: 69.41%, Val Loss: 0.7423, Val Acc: 69.68%\n",
            "Epoch [5/150]: Train Loss: 0.7393, Train Acc: 70.07%, Val Loss: 0.7252, Val Acc: 71.26%\n",
            "Epoch [6/150]: Train Loss: 0.7310, Train Acc: 70.54%, Val Loss: 0.7187, Val Acc: 71.70%\n",
            "Epoch [7/150]: Train Loss: 0.7265, Train Acc: 70.86%, Val Loss: 0.7136, Val Acc: 71.17%\n",
            "Epoch [8/150]: Train Loss: 0.7204, Train Acc: 70.90%, Val Loss: 0.7035, Val Acc: 71.68%\n",
            "Epoch [9/150]: Train Loss: 0.7137, Train Acc: 71.50%, Val Loss: 0.7009, Val Acc: 72.14%\n",
            "Epoch [10/150]: Train Loss: 0.7169, Train Acc: 71.47%, Val Loss: 0.6960, Val Acc: 72.25%\n",
            "Epoch [11/150]: Train Loss: 0.7077, Train Acc: 71.81%, Val Loss: 0.6933, Val Acc: 72.84%\n",
            "Epoch [12/150]: Train Loss: 0.7050, Train Acc: 72.08%, Val Loss: 0.6921, Val Acc: 72.21%\n",
            "Epoch [13/150]: Train Loss: 0.7072, Train Acc: 71.61%, Val Loss: 0.6917, Val Acc: 72.72%\n",
            "Epoch [14/150]: Train Loss: 0.7029, Train Acc: 71.99%, Val Loss: 0.6812, Val Acc: 72.26%\n",
            "Epoch [15/150]: Train Loss: 0.7046, Train Acc: 71.91%, Val Loss: 0.6867, Val Acc: 72.69%\n",
            "Epoch [16/150]: Train Loss: 0.7007, Train Acc: 72.02%, Val Loss: 0.6796, Val Acc: 73.42%\n",
            "Epoch [17/150]: Train Loss: 0.6951, Train Acc: 72.37%, Val Loss: 0.6788, Val Acc: 73.89%\n",
            "Epoch [18/150]: Train Loss: 0.7006, Train Acc: 72.23%, Val Loss: 0.6813, Val Acc: 73.15%\n",
            "Epoch [19/150]: Train Loss: 0.7001, Train Acc: 72.03%, Val Loss: 0.6729, Val Acc: 73.54%\n",
            "Epoch [20/150]: Train Loss: 0.6981, Train Acc: 72.29%, Val Loss: 0.6772, Val Acc: 73.00%\n",
            "Epoch [21/150]: Train Loss: 0.6972, Train Acc: 72.19%, Val Loss: 0.6734, Val Acc: 72.78%\n",
            "Epoch [22/150]: Train Loss: 0.6958, Train Acc: 72.33%, Val Loss: 0.6736, Val Acc: 73.63%\n",
            "Epoch [23/150]: Train Loss: 0.6949, Train Acc: 72.30%, Val Loss: 0.6742, Val Acc: 73.55%\n",
            "Epoch [24/150]: Train Loss: 0.6924, Train Acc: 72.37%, Val Loss: 0.6744, Val Acc: 73.85%\n",
            "Epoch [25/150]: Train Loss: 0.6922, Train Acc: 72.67%, Val Loss: 0.6745, Val Acc: 73.32%\n",
            "Epoch [26/150]: Train Loss: 0.6970, Train Acc: 72.32%, Val Loss: 0.6642, Val Acc: 73.56%\n",
            "Epoch [27/150]: Train Loss: 0.6923, Train Acc: 72.36%, Val Loss: 0.6730, Val Acc: 73.33%\n",
            "Epoch [28/150]: Train Loss: 0.6900, Train Acc: 72.33%, Val Loss: 0.6685, Val Acc: 74.03%\n",
            "Epoch [29/150]: Train Loss: 0.6910, Train Acc: 72.53%, Val Loss: 0.6652, Val Acc: 73.59%\n",
            "Epoch [30/150]: Train Loss: 0.6888, Train Acc: 72.71%, Val Loss: 0.6674, Val Acc: 73.92%\n",
            "Epoch [31/150]: Train Loss: 0.6918, Train Acc: 72.60%, Val Loss: 0.6672, Val Acc: 74.11%\n",
            "Epoch [32/150]: Train Loss: 0.6892, Train Acc: 72.37%, Val Loss: 0.6729, Val Acc: 73.35%\n",
            "Epoch [33/150]: Train Loss: 0.6952, Train Acc: 72.43%, Val Loss: 0.6642, Val Acc: 74.08%\n",
            "Epoch [34/150]: Train Loss: 0.6916, Train Acc: 72.39%, Val Loss: 0.6647, Val Acc: 74.30%\n",
            "Epoch [35/150]: Train Loss: 0.6865, Train Acc: 72.51%, Val Loss: 0.6672, Val Acc: 73.70%\n",
            "Epoch [36/150]: Train Loss: 0.6898, Train Acc: 72.49%, Val Loss: 0.6608, Val Acc: 73.49%\n",
            "Epoch [37/150]: Train Loss: 0.6867, Train Acc: 72.74%, Val Loss: 0.6640, Val Acc: 73.42%\n",
            "Epoch [38/150]: Train Loss: 0.6922, Train Acc: 72.45%, Val Loss: 0.6581, Val Acc: 74.31%\n",
            "Epoch [39/150]: Train Loss: 0.6888, Train Acc: 72.44%, Val Loss: 0.6638, Val Acc: 73.89%\n",
            "Epoch [40/150]: Train Loss: 0.6887, Train Acc: 72.39%, Val Loss: 0.6642, Val Acc: 73.68%\n",
            "Epoch [41/150]: Train Loss: 0.6880, Train Acc: 72.65%, Val Loss: 0.6626, Val Acc: 73.98%\n",
            "Epoch [42/150]: Train Loss: 0.6876, Train Acc: 72.71%, Val Loss: 0.6624, Val Acc: 73.86%\n",
            "Epoch [43/150]: Train Loss: 0.6874, Train Acc: 72.73%, Val Loss: 0.6615, Val Acc: 73.93%\n",
            "Epoch [44/150]: Train Loss: 0.6875, Train Acc: 72.51%, Val Loss: 0.6649, Val Acc: 73.47%\n",
            "Epoch [45/150]: Train Loss: 0.6894, Train Acc: 72.62%, Val Loss: 0.6680, Val Acc: 73.45%\n",
            "Epoch [46/150]: Train Loss: 0.6882, Train Acc: 72.76%, Val Loss: 0.6639, Val Acc: 73.97%\n",
            "Epoch [47/150]: Train Loss: 0.6876, Train Acc: 72.56%, Val Loss: 0.6607, Val Acc: 74.07%\n",
            "Epoch [48/150]: Train Loss: 0.6891, Train Acc: 72.52%, Val Loss: 0.6621, Val Acc: 74.03%\n",
            "Epoch [49/150]: Train Loss: 0.6904, Train Acc: 72.58%, Val Loss: 0.6595, Val Acc: 74.29%\n",
            "Epoch [50/150]: Train Loss: 0.6899, Train Acc: 72.66%, Val Loss: 0.6663, Val Acc: 73.36%\n",
            "Epoch [51/150]: Train Loss: 0.6874, Train Acc: 72.54%, Val Loss: 0.6614, Val Acc: 74.13%\n",
            "Epoch [52/150]: Train Loss: 0.6881, Train Acc: 72.63%, Val Loss: 0.6510, Val Acc: 74.13%\n",
            "Epoch [53/150]: Train Loss: 0.6852, Train Acc: 72.61%, Val Loss: 0.6605, Val Acc: 74.12%\n",
            "Epoch [54/150]: Train Loss: 0.6857, Train Acc: 72.54%, Val Loss: 0.6678, Val Acc: 74.06%\n",
            "Epoch [55/150]: Train Loss: 0.6863, Train Acc: 72.50%, Val Loss: 0.6628, Val Acc: 73.98%\n",
            "Epoch [56/150]: Train Loss: 0.6852, Train Acc: 72.78%, Val Loss: 0.6636, Val Acc: 73.37%\n",
            "Epoch [57/150]: Train Loss: 0.6876, Train Acc: 72.52%, Val Loss: 0.6579, Val Acc: 74.08%\n",
            "Epoch [58/150]: Train Loss: 0.6885, Train Acc: 72.54%, Val Loss: 0.6566, Val Acc: 74.60%\n",
            "Epoch [59/150]: Train Loss: 0.6906, Train Acc: 72.38%, Val Loss: 0.6609, Val Acc: 73.89%\n",
            "Epoch [60/150]: Train Loss: 0.6901, Train Acc: 72.36%, Val Loss: 0.6559, Val Acc: 74.10%\n",
            "Epoch [61/150]: Train Loss: 0.6877, Train Acc: 72.69%, Val Loss: 0.6593, Val Acc: 73.98%\n",
            "Epoch [62/150]: Train Loss: 0.6892, Train Acc: 72.50%, Val Loss: 0.6528, Val Acc: 75.11%\n",
            "Epoch [63/150]: Train Loss: 0.6852, Train Acc: 72.62%, Val Loss: 0.6591, Val Acc: 74.37%\n",
            "Epoch [64/150]: Train Loss: 0.6891, Train Acc: 72.33%, Val Loss: 0.6501, Val Acc: 74.75%\n",
            "Epoch [65/150]: Train Loss: 0.6875, Train Acc: 72.58%, Val Loss: 0.6540, Val Acc: 74.39%\n",
            "Epoch [66/150]: Train Loss: 0.6846, Train Acc: 72.81%, Val Loss: 0.6532, Val Acc: 74.13%\n",
            "Epoch [67/150]: Train Loss: 0.6853, Train Acc: 72.75%, Val Loss: 0.6520, Val Acc: 73.95%\n",
            "Epoch [68/150]: Train Loss: 0.6867, Train Acc: 72.53%, Val Loss: 0.6527, Val Acc: 74.11%\n",
            "Epoch [69/150]: Train Loss: 0.6876, Train Acc: 72.85%, Val Loss: 0.6604, Val Acc: 74.19%\n",
            "Epoch [70/150]: Train Loss: 0.6835, Train Acc: 72.80%, Val Loss: 0.6634, Val Acc: 73.86%\n",
            "Epoch [71/150]: Train Loss: 0.6913, Train Acc: 72.37%, Val Loss: 0.6543, Val Acc: 74.19%\n",
            "Epoch [72/150]: Train Loss: 0.6899, Train Acc: 72.49%, Val Loss: 0.6571, Val Acc: 73.94%\n",
            "Epoch [73/150]: Train Loss: 0.6883, Train Acc: 72.52%, Val Loss: 0.6631, Val Acc: 73.63%\n",
            "Epoch [74/150]: Train Loss: 0.6882, Train Acc: 72.78%, Val Loss: 0.6599, Val Acc: 74.03%\n",
            "Epoch [75/150]: Train Loss: 0.6878, Train Acc: 72.59%, Val Loss: 0.6625, Val Acc: 74.05%\n",
            "Epoch [76/150]: Train Loss: 0.6861, Train Acc: 72.94%, Val Loss: 0.6571, Val Acc: 74.66%\n",
            "Epoch [77/150]: Train Loss: 0.6917, Train Acc: 72.36%, Val Loss: 0.6511, Val Acc: 74.72%\n",
            "Epoch [78/150]: Train Loss: 0.6887, Train Acc: 72.36%, Val Loss: 0.6526, Val Acc: 74.47%\n",
            "Epoch [79/150]: Train Loss: 0.6851, Train Acc: 72.54%, Val Loss: 0.6544, Val Acc: 74.82%\n",
            "Epoch [80/150]: Train Loss: 0.6902, Train Acc: 72.64%, Val Loss: 0.6524, Val Acc: 74.38%\n",
            "Epoch [81/150]: Train Loss: 0.6912, Train Acc: 72.21%, Val Loss: 0.6519, Val Acc: 74.46%\n",
            "Epoch [82/150]: Train Loss: 0.6900, Train Acc: 72.46%, Val Loss: 0.6600, Val Acc: 74.02%\n"
          ]
        }
      ],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# Define transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(299),\n",
        "    transforms.ToTensor(),  # Converts PIL Image to Tensor.\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the images.\n",
        "])\n",
        "\n",
        "# Load full dataset\n",
        "full_dataset = datasets.ImageFolder('/content/Final-Project/Data', transform=train_transform)\n",
        "\n",
        "# Parameters\n",
        "k = 5  # Number of folds\n",
        "epochs = 150\n",
        "num_classes = len(full_dataset.classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# K-Folds cross-validator\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Set up for k-fold results\n",
        "fold_results = {}\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(full_dataset)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Define training and validation data loaders\n",
        "    train_subset = Subset(full_dataset, train_ids)\n",
        "    val_subset = Subset(full_dataset, val_ids)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    # Model setup\n",
        "    model = timm.create_model('inception_v4', pretrained=True)\n",
        "    num_features = model.last_linear.in_features  # Get the number of input features to the last layer\n",
        "\n",
        "    # Replace the last linear layer with a new one that includes dropout\n",
        "    model.last_linear = nn.Sequential(\n",
        "        nn.Dropout(p=0.3),  # Add dropout with a probability of 50%\n",
        "        nn.Linear(num_features, num_classes)  # Followed by a linear layer\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # Freeze all layers except the final classification layer\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.last_linear.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00005)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, targets in train_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100. * correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, targets in val_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        val_loss = running_loss / len(val_loader)\n",
        "        val_accuracy = 100. * correct / total\n",
        "        print(f'Epoch [{epoch+1}/{epochs}]: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save model if this is the best val accuracy in this fold\n",
        "        if epoch == 0 or val_accuracy > fold_results.get(fold, (0, 0))[1]:\n",
        "            torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')\n",
        "            fold_results[fold] = (val_loss, val_accuracy)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "# Print fold results\n",
        "for fold, (loss, accuracy) in fold_results.items():\n",
        "    print(f'Best Validation Loss for Fold {fold}: {loss:.4f}')\n",
        "    print(f'Best Validation Accuracy for Fold {fold}: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FJNBj_L_HEnF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNLuvYy0dNFSTh+FG1QiF6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}